{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308e0b8-1fc9-4c70-94e0-796ea1010b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Hyperparameters\n",
    "num_epochs = 3\n",
    "seed = 10\n",
    "total_vocab = 5000\n",
    "max_len = 50\n",
    "batch_size = 100\n",
    "\n",
    "#data pre-processing code\n",
    "#import required data\n",
    "dataset = pd.read_csv('Sentiment Analysis Dataset.csv', on_bad_lines = 'skip')\n",
    "#split dataset into training and testing data\n",
    "trn_text, tst_text, trn_sent, tst_sent = train_test_split(dataset['text'], dataset['sentiment'], train_size = .8,\n",
    "                                                          shuffle = True, random_state = seed, stratify = dataset['sentiment'])\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179dd980-8528-4b69-97f2-488ab4261d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare the tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words = total_vocab, oov_token = 0)\n",
    "#fit tokenizer vocabulary on the dataset\n",
    "tokenizer.fit_on_texts(trn_text)\n",
    "#convert dataset text into sequences\n",
    "trn_seq_raw = tokenizer.texts_to_sequences(trn_text)\n",
    "tst_seq_raw = tokenizer.texts_to_sequences(tst_text)\n",
    "#pad sequences to fixed length\n",
    "trn_seq = pad_sequences(trn_seq_raw, padding = 'post', maxlen = max_len)\n",
    "tst_seq = pad_sequences(tst_seq_raw, padding = 'post', maxlen = max_len)\n",
    "#write tokenizer to json file\n",
    "with open('vocabulary_tokenizer.json', 'w') as f:\n",
    "    f.write(tokenizer.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831c0ce-a289-48e2-9ef2-729920b5ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model as a hypermodel for hyperparameter tuning\n",
    "class LSTM_HModel(kt.HyperModel):\n",
    "    #new build function\n",
    "    def build(self, hp):\n",
    "        #declare length of recurrent and dense layers as a hyperparameters\n",
    "        recurrent_layer_dim = hp.Int(name = 'recurrent_layer_dim', min_value = 160, max_value = 240, step = 20)\n",
    "        dense_layer_dim = hp.Int(name = 'dense_layer_dim', min_value = 5, max_value = 25, step = 5)\n",
    "        model = keras.Sequential()\n",
    "        #embedding layer declaration\n",
    "        model.add(keras.layers.Embedding(\n",
    "            input_dim = total_vocab, \n",
    "            output_dim = recurrent_layer_dim,\n",
    "            input_length = max_len,\n",
    "            mask_zero = True))\n",
    "        #recurrent LSTM layer declaration\n",
    "        model.add(keras.layers.Bidirectional(keras.layers.LSTM(recurrent_layer_dim)))\n",
    "        #add dense layer\n",
    "        model.add(keras.layers.Dense(dense_layer_dim, activation = 'softmax'))\n",
    "        #final output layer\n",
    "        model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "        #compile with binary crossentropy, accuracy as metric\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        #overload fit function for potential optimization of batch_size or epochs\n",
    "        return model.fit(*args,\n",
    "                         batch_size = batch_size,\n",
    "                         epochs = num_epochs,\n",
    "                         **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e6b14-35ea-4845-b6bb-23231454a7be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#declare hyperparameters and hypermodel\n",
    "hyp = kt.HyperParameters()\n",
    "main = LSTM_HModel()\n",
    "#build the hypermodel instance\n",
    "model = main.build(hyp)\n",
    "#delcare the random_search instance goal is to optimize accuracy\n",
    "rand_srch = kt.RandomSearch(\n",
    "    hypermodel = main.build,\n",
    "    objective = \"val_accuracy\",\n",
    "    max_trials = 5,\n",
    "    executions_per_trial = 1,\n",
    "    overwrite = False,\n",
    "    directory = 'hyperparam_tuning')\n",
    "\n",
    "rand_srch.search(x = trn_seq, y = trn_sent, validation_data = (val_text, val_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d60c8e-db11-430d-8fd8-f77e8d4b1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the optimal hyperparameters nad print them\n",
    "opt_hyp = rand_srch.get_best_hyperparameters(1)\n",
    "print(opt_hyp[0].values)\n",
    "#build and fit a model to those parameters\n",
    "model = main.build(opt_hyp[0])\n",
    "main.fit(hyp, model, x = trn_seq, y = trn_sent, validation_data = (tst_seq, tst_sent))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7c7c0-23ca-4d83-874c-2d52eccb3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('OPT_LSTM_model___NO.h5', save_format = 'h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
